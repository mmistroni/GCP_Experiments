{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StockAndNewsAPIs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmistroni/GCP_Experiments/blob/master/StockAndNewsAPIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9xnZo-xamUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!pip install pandas-datareader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giosOkpiMPyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEJyyYSJaraF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib\n",
        "import json\n",
        "import pandas as pd\n",
        "from pandas.tseries.offsets import BDay\n",
        "import pandas_datareader.data as dr\n",
        "import numpy as np\n",
        "from datetime import datetime, date\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqrFVyKPlrdX",
        "colab_type": "text"
      },
      "source": [
        "<h2>Authenticate User </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXM3PKNDlvaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2XGV6H1qGF6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<h3>Loading Credentials</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU0JmFUBNRkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "def get_iexapi_keys():\n",
        "  with open('gdrive/My Drive/passwords/iexapi.keys') as f:\n",
        "    return f.readlines()[0]\n",
        "\n",
        "def get_nlp_service_keys():\n",
        "  with open('gdrive/My Drive/passwords/nlp.keys') as f:\n",
        "    return f.readlines()[0]\n",
        "\n",
        "def get_newsapi_keys():\n",
        "  with open('gdrive/My Drive/passwords/newsapi.keys') as f:\n",
        "    return f.readlines()[0]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx2JKDUVNdlh",
        "colab_type": "text"
      },
      "source": [
        "<h3>IEX API CALLS </h3>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e18EfrivqJ_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "\n",
        "token = get_iexapi_keys()\n",
        "\n",
        "def get_statistics(ticker):\n",
        "  base_url = 'https://cloud.iexapis.com/stable/stock/{symbol}/stats?token={token}&format=csv&filter=companyName,symbol,beta,day50MovingAvg,day200MovingAvg,month6ChangePercent,month3ChangePercent,month1ChangePercent'.format(token=token,symbol=ticker)\n",
        "  df = pd.read_csv(base_url)\n",
        "  df['Symbol'] = ticker\n",
        "  return df\n",
        "\n",
        "def get_historical_data(ticker, start, end):\n",
        "  df = get_statistics(ticker)\n",
        "  return df\n",
        "\n",
        "\n",
        "def get_all_stocks():\n",
        "  all_symbols_data = requests.get('https://cloud.iexapis.com/stable/ref-data/iex/symbols?token={token}'.format(token=token)).json()\n",
        "  return [d['symbol'] for d in all_symbols_data if d['isEnabled']and d['type'].lower() == 'cs']\n",
        "\n",
        "def get_all_us_stocks(security_type='cs'):\n",
        "  nyse_symbols = requests.get('https://cloud.iexapis.com/stable/ref-data/exchange/nys/symbols?token={token}'.format(token=token)).json()\n",
        "  #nas_symbols = requests.get('https://cloud.iexapis.com/stable/ref-data/exchange/nas/symbols?token={token}'.format(token=token)).json()\n",
        "  return [d['symbol'] for d in nyse_symbols  if d['type'].lower() == security_type]\n",
        "\n",
        "def get_all_etfs():\n",
        "  stocks= get_all_us_stocks()\n",
        "  return [d['symbol'] for d in stocks if d['type'].lower() == 'et']\n",
        "\n",
        "def get_all_stocks_data():\n",
        "  good_ones = get_all_etfs()\n",
        "  return map(lambda symbol: (symbol, get_historical_value(symbol)), good_ones)\n",
        "\n",
        "\n",
        "def get_all_exchanges():\n",
        "  return requests.get('https://cloud.iexapis.com/stable/ref-data/market/us/exchanges?token={token}'.format(token=token)).json()\n",
        "\n",
        "def get_latest_price(symbol):\n",
        "  base_url = \"https://cloud.iexapis.com/stable/stock/{ticker}/quote?token={token}&format=csv&filter=symbol,close\".format(token=token,ticker=symbol)\n",
        "  import requests\n",
        "  return pd.read_csv(base_url)\n",
        "\n",
        "def get_quote(symbol):\n",
        "  try:\n",
        "    historical_url = 'https://cloud.iexapis.com/stable/stock/{symbol}/quote/latestPrice?token={token}'.format(token=token,symbol=symbol)\n",
        "    return requests.get(historical_url).json()\n",
        "  except:\n",
        "    return -1\n",
        "\n",
        "def get_news(symbol, num_of_news):\n",
        "  try:\n",
        "    news_url = 'https://cloud.iexapis.com/stable//stock/{symbol}/news/last/{last}?token={token}'.format(symbol=symbol, last=num_of_news,\n",
        "                                                                                                        token=token)\n",
        "    return requests.get(news_url).json()\n",
        "  except Exception as e :\n",
        "    print('Excepiton for {}:{}'.format(symbol, str(e)))\n",
        "    return []\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzqkOp1BsG7g",
        "colab_type": "text"
      },
      "source": [
        "<h3> Yahoo API Calls </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bNDGbCNbX6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import pearsonr\n",
        "import requests\n",
        "\n",
        "def get_latest_price_yahoo(symbol, as_of_date):\n",
        "  try:#\n",
        "    print('--latest price for{}'.format(symbol))\n",
        "    res = dr.get_data_yahoo(symbol, as_of_date, as_of_date)[['Close']]\n",
        "    df['Symbol'] = symbol\n",
        "    return df\n",
        "  except Exception as e :\n",
        "    return pd.DataFrame(columns=[symbol])\n",
        "\n",
        "def get_historical_data_yahoo(symbol, start_dt, end_dt):\n",
        "  try: \n",
        "    end_date = date.today()\n",
        "    print('hist dat for:{}'.format(symbol))\n",
        "    data = dr.get_data_yahoo(symbol, start_dt, end_dt)[['Adj Close']]\n",
        "    df =  data.rename(columns={'Adj Close' : symbol})\n",
        "    return df\n",
        "  except Exception as e :\n",
        "    return pd.DataFrame(columns=[symbol])   \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lVNnbmlOiUe",
        "colab_type": "text"
      },
      "source": [
        "<h3> Getting Sentiment Analysis from Google </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCwPBOkWsapp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sentiment_from_google(content):\n",
        "  from google.cloud import language\n",
        "  from google.cloud.language import enums\n",
        "  from google.cloud.language import types\n",
        "\n",
        "  client = language.LanguageServiceClient()\n",
        "  document = types.Document(\n",
        "      content=clean_text,\n",
        "      type=enums.Document.Type.PLAIN_TEXT)\n",
        "\n",
        "  # Detects the sentiment of the text\n",
        "  sentiment = client.analyze_sentiment(document=document).document_sentiment\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjTo6XOKimci",
        "colab_type": "text"
      },
      "source": [
        "<h3> Google Language API </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-_MtxkZirQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_language_api(content):\n",
        "  import httplib2\n",
        "  import sys\n",
        "  from googleapiclient import discovery\n",
        "  from googleapiclient.errors import HttpError\n",
        "\n",
        "  discovery_url = 'https://{api}.googleapis.com/$discovery/rest?version={apiVersion}'\n",
        "\n",
        "  service = discovery.build(\n",
        "      'language', 'v1',\n",
        "      http=httplib2.Http(),\n",
        "      discoveryServiceUrl=discovery_url,\n",
        "      developerKey=get_nlp_service_keys(),\n",
        "  )\n",
        "  service_request = service.documents().annotateText(\n",
        "      body={\n",
        "          'document': {\n",
        "              'type': 'PLAIN_TEXT',\n",
        "              'content': content,\n",
        "          },\n",
        "          'features': {\n",
        "              'extract_syntax': True,\n",
        "              'extractEntities': True,\n",
        "              'extractDocumentSentiment': True,\n",
        "          },\n",
        "          'encodingType': 'UTF16' if sys.maxunicode == 65535 else 'UTF32',\n",
        "      })\n",
        "  try:\n",
        "      #print('************************')\n",
        "      #print('Retrieving sentiment for:{}'.format(content))\n",
        "      response = service_request.execute()\n",
        "      return  response['documentSentiment']['score']\n",
        "      \n",
        "  except HttpError as e:\n",
        "      response = {'error': e}\n",
        "      print('exception:{}'.format(str(e)))\n",
        "      return 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNkdWNJri04r",
        "colab_type": "text"
      },
      "source": [
        "<h3> Getting News from News API <h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg8OLEEKi5qY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def retrieve_all_news(symbol):\n",
        "  print('Retrieving all news for@{}'.format(symbol))\n",
        "  token = get_newsapi_keys()\n",
        "  all_news = 'https://newsapi.org/v2/everything?q={ticker}&apiKey={token}'.format(ticker=symbol, token=token)\n",
        "  data = requests.get(all_news).json()\n",
        "  res = data['articles']\n",
        "  return map(lambda data: data['content'], res)\n",
        "\n",
        "def calculate_sentiment(news_items):\n",
        "  res = map(lambda item: test_language_api(item), news_items)\n",
        "  all_news =  list(res)\n",
        "  pprint('Total:{}'.format(all_news))\n",
        "  return dict(total=sum(all_news), positive=[i for i in all_news if i > 0], negative=[i for i in all_news if i <0])\n",
        "\n",
        "def calculate_news_sentiment(ticker):\n",
        "  latest_news = list(retrieve_all_news(ticker))\n",
        "  return calculate_sentiment(latest_news)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVDGvM5Si0xS",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-1KZ5kLV98U",
        "colab_type": "text"
      },
      "source": [
        "<h3>Computing various  metrics </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFvySZQvWBiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we need to sort out which function we need.\n",
        "#1. calculate sharpe ratio  DONE\n",
        "#2. get month change percent m30\n",
        "#3. get full performance   DONE\n",
        "#4. get news  DONE\n",
        "#5 group together and compute\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "from math import sqrt\n",
        "\n",
        "def calculate_daily_returns(prices):\n",
        "  return prices.pct_change(1)\n",
        "\n",
        "def calculate_daily_cumulative_returns(daily_pc):\n",
        "  return (1 + daily_pc).cumprod()\n",
        "\n",
        "def compute_standard_deviation(daily):\n",
        "  return daily.loc[:,daily.columns[0]].std()\n",
        "\n",
        "def compute_sharpe_ratio(s_prices):\n",
        "  # This function should be used in final dataframe\n",
        "  # USE THSI FUNCTION\n",
        "  dret = calculate_daily_returns(s_prices)\n",
        "  avg = dret.loc[:,dret.columns[0]].mean()\n",
        "  std = compute_standard_deviation(dret)\n",
        "  return (sqrt(252) * avg) / std\n",
        "\n",
        "def compute_moving_averages(prices, day):\n",
        "  print('Computing moving avg for:{}'.format(day))\n",
        "  return prices.rolling(window=day).mean()\n",
        "\n",
        "def check_prices_vs_moving_averages(prices, day=30):\n",
        "  # This Function should be used  in final dataframe\n",
        "  ma30 = compute_moving_averages(prices, 30)\n",
        "  ticker_col = ma30.columns[0]\n",
        "  m30_col = '{}M30'.format(ticker_col)\n",
        "  ma30_renamed = ma30.rename({ticker_col: m30_col}, axis=1)\n",
        "  concats = pd.concat([prices, ma30_renamed], axis=1)\n",
        "  concats['AboveM30'] = concats[ticker_col] > concats[m30_col]\n",
        "  above_m30 = concats[concats['AboveM30'] == True]\n",
        "  total_prices = prices.shape[0]\n",
        "  total_m30 = above_m30.shape[0]\n",
        "  pcnt = 1.0*total_m30/total_prices\n",
        "  return pcnt\n",
        "\n",
        "def compute_performance(historical_df):\n",
        "  # Use this FUNCTION CALL\n",
        "  start = historical_df['AMZN'].values[0]\n",
        "  end = historical_df['AMZN'].values[-1]\n",
        "  print('Start:{}, End:{}'.format(start, end))\n",
        "  return end*1.0/start - 1\n",
        "\n",
        "def compute_metrics(prices):\n",
        "  ticker = prices.columns[0]\n",
        "  print('Computing Metrics for:{}'.format(ticker))\n",
        "  perf_dict = {}\n",
        "  print('Computing performance..')\n",
        "  perf_dict['Performance'] = compute_performance(prices)\n",
        "  print('Computing days above moving average..')\n",
        "  perf_dict['AboveMovingAvgPcnt'] = check_prices_vs_moving_averages(prices)\n",
        "  print('comuting sharpe ratio....')\n",
        "  perf_dict['SharpeRatio'] = compute_sharpe_ratio(prices)\n",
        "  print('Returning data...')\n",
        "  perf_dict['News_Sentiment'] = calculate_news_sentiment(ticker)\n",
        "  return perf_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTZu-ZatcHOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get yahoo historical data\n",
        "from datetime import date\n",
        "historical_df =  get_historical_data_yahoo('AMZN', date(2019,10,1), date.today())\n",
        "compute_metrics(historical_df)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5FrRLkbnuVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "historical_df.pct_change().values.sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNzQRMr6wTAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = historical_df['AMZN'].values[0]\n",
        "end = historical_df['AMZN'].values[-1]\n",
        "pcnt = end/start - 1\n",
        "print('Start:{}.end:{}.Change:{}'.format(start, end, pcnt))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7O0VTsLeRm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_statistics('AMZN') # this cost 5 messages per share"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXSJQ73Am2dC",
        "colab_type": "text"
      },
      "source": [
        "<h3> Reading source data and computing performance </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euF2kHI-n4qf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_date_ranges():\n",
        "  end_date = date.today()\n",
        "  start_date = end_date - BDay(30)\n",
        "  return start_date, end_date\n",
        "\n",
        "def compute_performance(start_dt, end_dt, ticker):\n",
        "  try:\n",
        "    # Here we need to compute, in addition to performance, also sharpe ratio etc.\n",
        "    import time\n",
        "    historical_df =  get_historical_data_yahoo(ticker, start_dt, end_dt)\n",
        "    print('hist df cols:{}'.format(historical_df.columns))\n",
        "    latest_df = get_latest_price_yahoo(ticker, end_dt)\n",
        "    print('latest df ocls:{}'.format(latest_df.columns))\n",
        "    merged = pd.merge(historical_df, latest_df, how='inner' , on=ticker)\n",
        "    return merged\n",
        "  except Exception as e:\n",
        "    print('Exception:{}'.format(str(e)))\n",
        "    print('Unable to find data for {}:{}'.format(ticker,str(e)))\n",
        "    \n",
        "def find_best_performing(start_dt, end_dt):\n",
        "  print('Finding Best Performing Stocks between:{}-{}'.format(start_dt, end_dt))\n",
        "  symbols = ['ABBV'] #get_all_us_stocks()[0:10]\n",
        "  print('Now we have to source data for:{}'.format(len(symbols)))\n",
        "  dfs = (compute_performance(start_dt, end_dt, symbol) for symbol in symbols)\n",
        "  filtered = (df for df in dfs if df is not None)\n",
        "  all_data = pd.concat(filtered)\n",
        "  return pd.merge(nyse_df, all_data, how='inner', on='Symbol' )[['Symbol', 'Name', 'Sector', 'industry', 'companyName','close', \n",
        "       'month1ChangePercent','month3ChangePercent', 'month6ChangePercent',  'day200MovingAvg', 'day50MovingAvg']]\n",
        "\n",
        "  \n",
        "\n",
        "start_dt, end_dt = get_date_ranges()\n",
        "perf_df = find_best_performing(start_dt, end_dt)\n",
        "# Sorting \n",
        "#perf_df.sort_values(by=['month1ChangePercent'], inplace=True, ascending=False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBFhOUKUQ3Lg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_historical_data_yahoo('AMZN', date(2019,11,24), date.today())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GGRcMTbwhCj",
        "colab_type": "text"
      },
      "source": [
        "<h2> TODO: Fetch news for every ticker and find out sentiment. then build a dataframe of symbol, positive news, performance </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEBMn4Zlkjgf",
        "colab_type": "text"
      },
      "source": [
        "<h3> Group by sector, to find best performers </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BrXBHSsqa9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res = perf_df[['industry', 'month1ChangePercent','month3ChangePercent', ]].groupby(['industry']).mean().sort_values(by=['month1ChangePercent','month1ChangePercent'], ascending=False)\n",
        "res.head(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNrTkBIcADGb",
        "colab_type": "text"
      },
      "source": [
        "<p> Testing all stocks in portfolio </p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oildRTtsAIh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "portfolio_shares = ['ADAC', 'AMBS', 'AMZN', 'AZFL', 'ARSC', 'AAPL', 'APTY',\n",
        "                    'BTCS', 'BRK-B', 'CRNT', 'CRLBF', 'XOM', 'HAON', 'AGEEF',\n",
        "                    'HMNY', 'JNJ', 'LEMIF', 'NXTTF', 'NVCN', 'RNVA', 'TORC',\n",
        "                    'RTRX', 'VALE', 'VZ', 'DGP', 'RUSL', 'REMX', 'TVIX' ]\n",
        "\n",
        "all_shares = get_all_stocks()\n",
        "res = map(lambda symbol:(symbol, symbol in all_shares), shares)\n",
        "invalid = [tpl for tpl in res if not tpl[1]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT-4mE-grwDa",
        "colab_type": "text"
      },
      "source": [
        "<h3> Performance Functions </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksISPaesqPEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_correlation_all(vix, all_stocks):\n",
        "  all_df = [vix]\n",
        "  res= [vals for _, vals in all_stocks if vals.shape[0] == vix_vals.shape[0]]\n",
        "  res.append(vix)\n",
        "  all_data = pd.concat(res, axis=1)\n",
        "  return all_data.corr('pearson')\n",
        "\n",
        "def calculate_portfolio_correlation(all_stocks):\n",
        "  res= [vals for _, vals in all_stocks if vals.shape[0] > 2]\n",
        "  all_data = pd.concat(res, axis=1)\n",
        "  return all_data.corr('pearson')\n",
        "\n",
        "\n",
        "def calculate_correlation(vix, all_stocks):\n",
        "  result = []\n",
        "  best = 0\n",
        "  for symbol, vals in all_stocks:\n",
        "    if vals.shape[0] == vix.shape[0]:\n",
        "      concats  = pd.concat([vix, vals], axis = 1)\n",
        "      corr_matrix = concats.corr(method='pearson')\n",
        "      corr_with_vix = corr_matrix.loc['^VIX'][1]\n",
        "      if corr_with_vix > 0 and corr_with_vix > best:\n",
        "        print('New Corr with {}:{}'.format(symbol, corr_with_vix))\n",
        "        best = corr_with_vix\n",
        "  return best\n",
        "\n",
        "def _get_most_correlated(result_df):\n",
        "  df = result_df[['^VIX']]\n",
        "  bad_df = df.index.isin(['^VIX'])\n",
        "  return df[~bad_df]\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpVVvwqxGYc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#vix_vals = get_historical_value('^VIX')\n",
        "\n",
        "all_stocks_data = map(lambda symbol: (symbol, get_historical_value(symbol)), portfolio_shares)\n",
        "best = calculate_portfolio_correlation(all_stocks_data)      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNX-4-970hiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res = _get_most_correlated(best)\n",
        "sorted_df = res.sort_values('^VIX', ascending=False)\n",
        "sorted_df.head(10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ-fsQbw3o53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = sorted_df.iloc[0]\n",
        "\n",
        "print(idx.values.tolist()[0])\n",
        "print(sorted_df.index[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o3uRW6bjOTA",
        "colab_type": "text"
      },
      "source": [
        "<h3> Getting All US Stocks </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COGKQLeXjThy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_stocks = get_all_us_stocks()\n",
        "print('We got to find:{}'.format(len(all_stocks)))\n",
        "all_stocks[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mN55XgONj1J",
        "colab_type": "text"
      },
      "source": [
        "<h3> REtrieving News </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-WGaL7jNmVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_iexapi_news(ticker):\n",
        "  all_news = map(lambda ticker : get_news(ticker, 100), all_stocks) # will not work.intraday news. \n",
        "  data = list(all_news)\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}